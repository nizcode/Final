---
title: "Final Project"
author: "Neil Farrugia 17336831"
date: "14/11/2021"
output: html_document
---

<style type="text/css">

body{ /* Normal  */
      font-size: 12px;
  }
td {  /* Table  */
  font-size: 8px;
}
h1.title {
  font-size: 38px;
  color: DarkRed;
}
h1 { /* Header 1 */
  font-size: 28px;
  color: DarkBlue;
}
h2 { /* Header 2 */
    font-size: 22px;
  color: DarkBlue;
}
h3 { /* Header 3 */
  font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
}
code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 14px;
}
</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Part 1: Analysis

## Reading in the data
```{r}
#https://www.kaggle.com/natashasavc/ie-students-winemag-analysis
#another idea is to look at the years!
#ie. year and price/points
library(tidyverse)
library(ggplot2)

#read in the data
df <- read.csv("winemag-data-130k-v2.csv")
nas <- apply(apply(df, 2, is.na),2,sum)
#Quick anaylsis on missing values
barplot(nas,main="Missing values investigation",
        ylab="num of NAs",las=2)


on <- dim(df)[1]
vn <- dim(df)[2]
sum <- c("n observation"=on, "n variables" = vn)
print(sum)


```

From this brief missing values exploration, we can see that there is only 1 column that contains missing values - points column. With just over 8000 missing values.

The total number of observation is `r on` and the total number of variables is `r vn`.

Due to the fact that there is a high number of observation in this data set, completely ommitting the rows with a missing price will still allow plenty of data points to draw conclusions from.

```{r}
df1 <- df[complete.cases(df),]
```
## Analysis on distrubition of prices and points

```{r}
#Summary of price
summary(df1$price)
#Interqurtile range

#Hypothesis - Very few wines are above the $200/300 mark
#These next few lines - finds the range of bottles at certain price that covers
#99.5% of the data set
tp <- table(df1$price)
p99 <- ceiling(length(df1$price)*0.995)
rngp99 <-(p99-20):(p99+20) #Give or take $20

tpRS <- cumsum(tp)
pless <- tp[which(tpRS%in%rngp99)]
plessLabel <- labels(pless[length(pless)])

print(paste0("$1 to $",plessLabel," covers 99.5% of the total dataset"))
```


As you can see from the calculations above there is a really big range of prices. Due to the fact that under 99.5% of the wines in this data set are below $225, anything above 225 will be removed. Removing these outliers will allow for a better a visualisations of the analysis and distributions.

```{r}
df1 <- df1[which(df1$price < as.numeric(plessLabel)),]

```


```{r}
#Points and price analysis
avg <- mean(df1$points)
median <- median(80:100)
colors <- c("mean" = "green", "median" = "orange")
#colors <- c("mean" = "blue", "mode" = "red", "median" = "orange")
ggplot(df1,aes(x=points))+
  geom_histogram(binwidth = 1,aes(y=..density..))+
  #geom_vline(aes(xintercept = mode,color="mode"))+
  geom_vline(aes(xintercept = avg,color="mean"))+
  geom_vline(aes(xintercept = median,color="median"))+
  labs(title="Distribution of points",
         color = "Legend") +
 stat_function(fun=dnorm, args = list(mean = mean(df1$points), sd= sd(df1$points)), col = 'blue')+
 scale_color_manual(values = colors)



avgP <- mean(df1$price)
#modeP <- mode(df1$price)
medianP <- median(df1$price) 
sumP <- c("mean Price" = avgP, "median Price" = medianP)


#look at wine prices, for different countries
#only look at top 10 countries with most wines
#only max 200euro
ctab <- sort(table(df1$country), decreasing = T)
ctab10 <- labels(ctab[1:10])[[1]]
df10 <- df1[which(df1$country%in%ctab10),]
ggplot(df1,aes(x=price))+
  geom_histogram(binwidth = 5,aes(y=..density..))+
  #geom_vline(aes(xintercept = mode,color="mode"))+
  geom_vline(aes(xintercept = avgP,color="mean"))+
  geom_vline(aes(xintercept = medianP,color="median"))+
  labs(title="Distribution of prices",
         color = "Legend") +
 stat_function(fun=dnorm, args = list(mean = mean(df1$price), sd= sd(df1$price)), col = 'blue')+
 scale_color_manual(values = colors)


    
```
It was interesting to note the distribution of prices and points.
As you can see the points follow a normal distribution. Where as the prices the distribution is very much skewed to the right.


### Analyisis on wine prices in different top 10 countries

```{r}
length(unique(df1$country))

```
There are a total of 43 countries. For ease of visualisation, only the ten countries that have the most observation will be used in this price analysis.

```{r}
topCon <- labels(sort(table(df1$country),decreasing = T)[1:11])[[1]]
df102 <- df10[which(df10$country %in% topCon),] 
topCon
```
These top 11 countries are also among st the top wine producing countries according to this website, https://worldpopulationreview.com/country-rankings/wine-producing-countries

Analysis on the price of wine in these 11 countries:

```{r}
avgPvC<-aggregate(df102$price,by=list(country=df102$country),FUN=mean)
ggplot(df102,aes(x = country,y=price))+
  geom_boxplot(fill = "red")+
  labs(title="Distribution Prices per Country",
         y = "Prices", x = "Country")
  


```

 What conlclusions can be drawn from these boxplot:
 1. US,Italy,France,Germany and Italy all seem to produce more expensive wines in comparison to the others.
 2. Chile has the least amount of variance and also produces the cheapest wines, with Argentina not far off.
 3. Italy has the biggest range of prices.
 4. Spain and Portugal are very similar
 
 
## Analysis on the correlation between price and points
```{r}
ggplot(df10,aes(x=points,y=price))+
  geom_point(position="jitter",color = "purple") + 
  geom_smooth(method="lm", se=F)+
  labs(title = "Price vs Points")
  
print(paste("Correlation coefficient=",round(cor(df10$points,df10$price),2)))
```
The correlation is positive, and it is of medium strength.

```{r}
#https://cran.r-project.org/web/packages/tokenizers/vignettes/introduction-to-tokenizers.html
library(stopwords)
library(ggwordcloud)
library(tokenizers)
df3 <- df1[1:1000,]
#str_split(df3$description," ")
rev <- df1$description
obviousWords <- c("wine","flavors","now","nose","well","show","like","aroma","drink","body","offer","vineyard","sauvignon","open","mouth","blanc","cabernet")
stopwords = c(stopwords::stopwords("en"),tolower(df3$variety),df3$province,df3$country,obviousWords)
#wds <- unlist(tokenize_ngrams(rev,n=2,n_min=1,stopwords=stopwords))
wds <- unlist(tokenize_word_stems(rev,stopwords=stopwords))
wdsdf <- data.frame(sort(table(wds),decreasing=T))
ggplot(wdsdf[1:100,],
       aes(label=wds,color = factor(sample.int(10, 100, replace = TRUE)),size=Freq))+
  geom_text_wordcloud() +
  labs(title = "Word Cloud for dataset")+
  theme_minimal()
  
#grepl("Red",df3$variety)
  
```

```{r}
#split top 10% wines and bottom 10% wines
library(gridExtra)

dfG <- df1[which(df1$points>96),]
dfB <- df1[which(df1$points<81),]

revG <- dfG$description
revB <- dfB$description
obviousWords <- c("wine","flavors","now","nose","well","show","like","aroma","drink","body","offer","vineyard","sauvignon","open","mouth","blanc","cabernet")
stopwords = c(stopwords::stopwords("en"),tolower(df3$variety),df3$province,df3$country,obviousWords)
#wds <- unlist(tokenize_ngrams(rev,n=2,n_min=1,stopwords=stopwords))
wG<-unlist(tokenize_word_stems(revG,stopwords=stopwords))
wB <- unlist(tokenize_word_stems(revB,stopwords=stopwords))
wdsG <- data.frame(sort(table(wG),decreasing = T))
wdsB <- data.frame(sort(table(wB),decreasing = T))

Gp <- ggplot(wdsG[1:30,],
       aes(x=wG, y=Freq,fill=wG))+
  geom_bar(stat="identity") +
  guides(fill = F)+
  xlab(NULL)+
  coord_flip()+
  labs(title = "Bar Chart for Best")+
  theme_minimal()
Bp <- ggplot(wdsB[1:30,],
       aes(x=wB, y=Freq,fill=wB))+
  geom_bar(stat="identity") +
  xlab(NULL)+
  guides(fill = F)+
  coord_flip()+
  labs(title = "Bar Chart for Worst")+
  theme_minimal()
grid.arrange(Bp,Gp,ncol=2)

```
It was very interesting comparing the words used in the best wines in comparison to the worst wines (relative in the datset).

When describing the worst wines, they seem to focus more on more general terms, fruit, taste, smell, bitter, cherry, aroma. However the words that are used to describe the best wines are more precise, more pronounced and more descriptive. Such as, rich, structure, beautiful, power, balanced, complex.

Tannin which is top 2nd most frequent used in the best wines is not used in the worst wines. Tannin is a very important concept when it comes to wine in general. The fact that it is not used in describing ...........

Moreover, the words age and year were used quite frequently when describing the best wines. Whereas it wasn't use much at all it seems in the worst wnes. Meaning that age of the wine (most likely how old it is) is an important factor when it comes to the quality of the best wines.
``


# Part 2
## Tokenizer package analysis

```{r}
library(tidytext)
library(dplyr)
library(stopwords)
library(tokenizers)
df0 <- read.csv("wiki_movie_plots_deduped.csv")
df0 <- df0[,c('Title','Genre','Plot')]

startTy <- Sys.time()
word_df1<-unnest_tokens(df0,word,Plot)%>%
  anti_join(stop_words)%>%
  count(word,sort=TRUE)
endTy <- Sys.time()

startTo <- Sys.time()
word_df2<-tokenize_words(df0$Plot, stopwords = stopwords::stopwords("en"),strip_numeric = TRUE)
word_df2<-data.frame(sort(table(word_df2),decreasing=TRUE))
endTo <- Sys.time()




print(paste("It took the tidytext package", round(endTy - startTy,2),"seconds","to perform tokenisation and a count of the tokens", "where the tokenizers package takes",round(endTo - startTo,2),"seconds"))
```

This shows the speed of the tokenizer package, yet it has some very useful add ons,that tidy tyext does not have.
For example:
You can tokenize group of words,
tokenize_ngrams(words, n = 5, n_min = 2), grouping with at most n words and at least n_min words.
```{r}
dfc <- df0[grepl("romance",df0$Genre,fixed=TRUE),]

word_df<-unlist(tokenize_ngrams(df0$Plot, n=2, n_min=2,stopwords = stopwords::stopwords("en")))#,strip_numeric = TRUE)
word_df<-data.frame(sort(table(word_df),decreasing=TRUE))

ggplot(word_df[1:20,],
       aes(label=word_df,color = factor(sample.int(10, 20, replace = TRUE)),size=Freq))+
  geom_text_wordcloud() +
  labs(title = "Word Cloud for dataset")+
  theme_minimal()




```
This analysis performed here using the ngram function proves exactly its usefulness. This is because imagine you solely analysed 1 word tokens, you may find a high number of "new" tokens and "york" possibly as well. These tokens went taken independtly are meaningless, it is only when you put them together that you can form some sort of conclusion, ie. a lot of romance movie happen in New York.

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
